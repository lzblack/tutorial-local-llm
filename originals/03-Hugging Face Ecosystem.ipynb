{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92e2b000-2c51-43c0-98e5-14149c7b0fa6",
   "metadata": {},
   "source": [
    "# 3 - Hugging Face Model Repository"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c947ce9-db64-4117-9a90-9ba230cbe88c",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <img src=\"../img/HF-logo-horizontal.png\" width=600>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04547fb6-5719-43b3-a4b7-0de93c11ac91",
   "metadata": {},
   "source": [
    "First, it's important to understand that [Hugging Face https://huggingface.co/](https://huggingface.co/) provides a [vast ecosystem](https://huggingface.co/welcome) for Machine Learning & AI practitioners:\n",
    "\n",
    "* A [repository](https://huggingface.co/models) of ML/AI models\n",
    "* A suite of [software libraries](https://huggingface.co/docs) (mostly in Python and JS)\n",
    "* Over 500k [datasets](https://huggingface.co/datasets) suitable for training or testing\n",
    "* ML/AI [training content](https://huggingface.co/learn)\n",
    "* ML/AI [research publications](https://huggingface.co/papers)\n",
    "* Model hosting UI in [HF Spaces](https://huggingface.co/spaces)\n",
    "* Personal & Organizational [Hugging Face Hub](https://huggingface.co/docs/hub/en/index) to coordiante your use of AI models (local or cloud)\n",
    "* Community [forum](https://discuss.huggingface.co/) for discussion and a [blog](https://huggingface.co/blog)\n",
    "\n",
    "You should definitely explore it more, but for us we'll just focus on the [model repository https://huggingface.co/models](https://huggingface.co/models)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d6ba6ab-3d68-477f-9574-3f0279d8bd1c",
   "metadata": {},
   "source": [
    "## 3.1 Find a GGUF model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b82258b5-8da5-468e-ad44-e1bbc30ad733",
   "metadata": {},
   "source": [
    "[*GGUF*](https://huggingface.co/docs/hub/en/gguf) (GPT-Generated Unified Format) model files provide a one-file version of a model that can be used with Ollama, LLama.cpp, and other LLM hosting frameworks.  There are other model formats, such as [*Safetensors*](https://huggingface.co/docs/safetensors/en/index), but they require more steps to work with for self-hosting, so we'll stick with GGUF for now.\n",
    "\n",
    "Hugging Face has over [150k GGUF format models](https://huggingface.co/models?library=gguf), so you need to be selective in which ones are suitable for your needs.\n",
    "\n",
    "[https://huggingface.co/models?library=gguf](https://huggingface.co/models?library=gguf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e26051-e75a-4831-a062-cf72aaf347dd",
   "metadata": {},
   "source": [
    "For our purposes, let's look at the [Phi3 Mini 4B parameter model](https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-gguf) from Microsoft.\n",
    "\n",
    "You can right away start downloading the Q4 GGUF file which is about 2.4GB, and cross your fingers it downloads while we read through the model card.  You should download this file to the `modelfiles` folder in the tutorial repository.\n",
    "\n",
    "[https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-gguf/tree/main](https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-gguf/tree/main)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27218861-f2b7-43f3-877d-5270849b25ec",
   "metadata": {},
   "source": [
    "## 3.2 Create a `Modelfile` for the GGUF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d2bb7b-cde0-4c63-ba6c-f52f0b938b5f",
   "metadata": {},
   "source": [
    "The *Model Card* for Phi3 Mini specified that the the `Modelfile` can be downloaded with this command:\n",
    "\n",
    "```\n",
    "hf download microsoft/Phi-3-mini-4k-instruct-gguf Modelfile_q4 --local-dir\n",
    "```\n",
    "\n",
    "However if you look in the *\"Files and versions\"* area of the model repository you'll also see you can directly download the file from the web interface.  A version of it is already available in the `model_import` folder in the tutorial repository.\n",
    "\n",
    "`phi3-4b.modelfile`:\n",
    "```\n",
    "FROM ./Phi-3-mini-4k-instruct-q4.gguf\n",
    "\n",
    "TEMPLATE \"\"\"<s>{{ if .Prompt }}<|user|>\n",
    "{{ .Prompt }}<|end|>\n",
    "{{ end }}<|assistant|>\n",
    "{{ .Response }}<|end|>\"\"\"\n",
    "\n",
    "PARAMETER stop <|endoftext|>\n",
    "PARAMETER stop <|assistant|>\n",
    "PARAMETER stop <|end|>\n",
    "\n",
    "PARAMETER num_ctx 4096\n",
    "```\n",
    "\n",
    "Now create the Ollama model from the modelfile:\n",
    "```\n",
    "ollama create phi3-mini:4b -f phi3-4b.modelfile\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e4a2689-8663-42df-bc16-d12e86ff4253",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-08T21:57:47.237795Z",
     "iopub.status.busy": "2025-12-08T21:57:47.236512Z",
     "iopub.status.idle": "2025-12-08T21:57:47.719688Z",
     "shell.execute_reply": "2025-12-08T21:57:47.719252Z",
     "shell.execute_reply.started": "2025-12-08T21:57:47.237710Z"
    }
   },
   "outputs": [],
   "source": [
    "import ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "460749bc-7a00-473a-9260-ecdd6efdfbb3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-08T21:57:51.523537Z",
     "iopub.status.busy": "2025-12-08T21:57:51.523238Z",
     "iopub.status.idle": "2025-12-08T21:58:03.674829Z",
     "shell.execute_reply": "2025-12-08T21:58:03.673961Z",
     "shell.execute_reply.started": "2025-12-08T21:57:51.523513Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.79 ms, sys: 6.93 ms, total: 9.72 ms\n",
      "Wall time: 12.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "response = ollama.generate(model='phi3-mini:4b',\n",
    "                           prompt='What are three things I should do this week?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ffd07e7e-885e-4403-88b5-8e1bec784a99",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-08T21:58:05.166938Z",
     "iopub.status.busy": "2025-12-08T21:58:05.166312Z",
     "iopub.status.idle": "2025-12-08T21:58:05.174644Z",
     "shell.execute_reply": "2025-12-08T21:58:05.173410Z",
     "shell.execute_reply.started": "2025-12-08T21:58:05.166896Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Here are three productive activities you can consider doing within the next week:\n",
      "\n",
      "1. Learn a new skill or hobby: Dedicate some time to learn something that interests you, whether it's cooking, painting, playing an instrument, coding, or learning a foreign language. Pick one specific area and set aside dedicated time each day for practice. There are numerous resources available online like tutorials, courses on platforms such as Udemy, Coursera, or Skillshare that cater to different skill sets.\n",
      "\n",
      "2. Organize your personal life: Set goals around improving various aspects of your personal well-being and daily routines. This can include setting a fitness routine, meal planning, reorganizing your living space, decluttering digital files or implementing a better sleep schedule. These changes can significantly improve overall quality of life, productivity, and mental health.\n",
      "\n",
      "3. Contribute to the community: Look for ways you can give back within your local area. This might include volunteering at a food bank, participating in neighborhood cleanups, joining a book club or mentorship program, supporting a charity by hosting a fundraising event or donating time and/or resources. Not only will this help those in need, but it can also provide you with new friendships and experiences, enhancing your own well-being along the way.\n",
      "\n",
      "Remember that setting specific goals for each task is helpful to stay motivated, track progress, and achieve results within a week.\n"
     ]
    }
   ],
   "source": [
    "print(response.response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1389852a-33d4-4410-b47d-6095e9b48b53",
   "metadata": {},
   "source": [
    "## 3.3 Serve GGUF model with Llama.cpp\n",
    "\n",
    "There are several tools you can use for serving local LLMs.  Ollama is a versatile all-in-one framework with UI, CLI, API, and Python libraries that can support both local LLMs and cloud-hosted LLMs, with no constraints on personal or professional/commercial use.\n",
    "\n",
    "However Llama.cpp is a performant and more production-oriented LLM server (and Ollama is built on top of it).\n",
    "\n",
    "```\n",
    "llama-server -m ./Phi-3-mini-4k-instruct-q4.gguf --jinja -c 0 --host 127.0.0.1 --port 8080```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95743eef-c90d-4ff2-8e1d-0645812d8441",
   "metadata": {},
   "source": [
    "Llama.cpp will provide a web interface to the served model(s), in this case available at:\n",
    "\n",
    "[http://127.0.0.1:8080](http://127.0.0.1:8080)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8effd2-97dc-4da7-8e4f-26fa323cc142",
   "metadata": {},
   "source": [
    "## 3.4 Use the `openai` Python library to access the local LLM server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7b2f5b5-8ce2-400a-948f-888133970e66",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-08T22:00:57.586800Z",
     "iopub.status.busy": "2025-12-08T22:00:57.585974Z",
     "iopub.status.idle": "2025-12-08T22:00:57.962065Z",
     "shell.execute_reply": "2025-12-08T22:00:57.961584Z",
     "shell.execute_reply.started": "2025-12-08T22:00:57.586757Z"
    }
   },
   "outputs": [],
   "source": [
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3cf2e855-0e30-4c8c-9349-1fc2e7ffe791",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-08T22:01:01.718093Z",
     "iopub.status.busy": "2025-12-08T22:01:01.717671Z",
     "iopub.status.idle": "2025-12-08T22:01:01.748144Z",
     "shell.execute_reply": "2025-12-08T22:01:01.746938Z",
     "shell.execute_reply.started": "2025-12-08T22:01:01.718063Z"
    }
   },
   "outputs": [],
   "source": [
    "client = OpenAI(base_url=\"http://127.0.0.1:8080/v1\",\n",
    "                # no API key needed when accessing Llama.cpp LLM server\n",
    "                api_key=\"dummy\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3e4a54d1-074f-4275-a456-bbedcef0f7ae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-08T22:01:03.743433Z",
     "iopub.status.busy": "2025-12-08T22:01:03.743089Z",
     "iopub.status.idle": "2025-12-08T22:01:03.747101Z",
     "shell.execute_reply": "2025-12-08T22:01:03.746049Z",
     "shell.execute_reply.started": "2025-12-08T22:01:03.743406Z"
    }
   },
   "outputs": [],
   "source": [
    "messages = [\n",
    "        dict(role=\"system\", content=\"You are a helpful assistant.\"),\n",
    "        dict(role=\"user\",   content=\"What are the top 3 things to know about LLMs?\")\n",
    "            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3414fd13-89b0-4e7a-82e7-7fe8c7019bd6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-08T22:01:13.913866Z",
     "iopub.status.busy": "2025-12-08T22:01:13.913401Z",
     "iopub.status.idle": "2025-12-08T22:01:36.768805Z",
     "shell.execute_reply": "2025-12-08T22:01:36.768365Z",
     "shell.execute_reply.started": "2025-12-08T22:01:13.913834Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 80 ms, sys: 29 ms, total: 109 ms\n",
      "Wall time: 22.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "response = client.chat.completions.create(\n",
    "    model=\"dummy\",  # Llama.cpp can only serve one model at a time, so this is ignored\n",
    "    messages=messages\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e9c794e-26de-4a27-9066-a81828024cd7",
   "metadata": {},
   "source": [
    "The actual response text is buried several layers in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "32837a1b-7921-49a0-b726-4b4851fc8ba7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-08T22:02:10.019226Z",
     "iopub.status.busy": "2025-12-08T22:02:10.018731Z",
     "iopub.status.idle": "2025-12-08T22:02:10.026648Z",
     "shell.execute_reply": "2025-12-08T22:02:10.025340Z",
     "shell.execute_reply.started": "2025-12-08T22:02:10.019193Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Large Language Models (LLMs) are a type of Artificial Intelligence (AI) models that have made significant advancements in natural language processing (NLP) and understanding. Here are the top three things you should know about LLMs:\n",
      "\n",
      "1. Basics and Components:\n",
      "   a. Definition: Large Language Models are deep learning-based models that use neural network architectures, such as Transformer and its variants (e.g., GPT-3, BERT, T5, etc.), to process and analyze human language. These models are designed to understand, interpret, and generate human language in a way that is coherent, relevant, and contextually appropriate.\n",
      "\n",
      "   b. Components: LLMs are composed of three main components - an input layer, hidden layers, and an output layer. The input layer processes the input text or sequence of tokens, while hidden layers (stacked multiple layers) capture the hierarchical and contextual relationships between tokens, and the output layer generates the desired output, such as a continuation of the given text or a classification.\n",
      "\n",
      "2. Capabilities and Applications:\n",
      "   a. Language understanding and generation: LLMs excel at tasks like text comprehension, translation, summarization, question answering, and text completion. They can also generate human-like text, making them suitable for applications such as chatbots, content creation, and automated writing assistance.\n",
      "\n",
      "   b. Transfer learning: LLMs can be pre-trained on large datasets and then fine-tuned for specific tasks, such as sentiment analysis, named entity recognition, and topic classification. This process, known as transfer learning, allows LLMs to generalize well to a variety of natural language processing tasks.\n",
      "\n",
      "   c. Multimodal capabilities: More recent LLMs, like Google's Multimodal BERT, have been designed to process not only text but also images, allowing them to better understand and generate content that combines both types of data.\n",
      "\n",
      "3. Limitations and Challenges:\n",
      "   a. Contextual understanding: Although LLMs have made significant progress in understanding context, they still struggle to grasp nuanced meanings, sarcasm, and other aspects of human language. This can lead to unexpected, off-topic, or even inappropriate outputs.\n",
      "\n",
      "   b. Bias and fairness: LLMs are trained on large, real-world datasets, which means they may inadvertently learn and perpetuate the biases present in the training data. This can result in unfair or discriminatory behavior in the generated outputs, making it essential to carefully curate and preprocess datasets and continuously evaluate and mitigate biases.\n",
      "\n",
      "   c. Resource-intensive: Training and deploying LLMs requires significant computational resources, including high-end GPUs and large amounts of data. This can limit the accessibility of these models to individuals, small businesses, and organizations without adequate resources.\n",
      "\n",
      "In conclusion, Large Language Models have revolutionized NLP and continue to provide impressive capabilities in understanding and generating human-like text. However, it's essential to be aware of the limitations and challenges associated with these models, as well as the potential for biases in their outputs.\n"
     ]
    }
   ],
   "source": [
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "491358c8-a9ad-4ea1-bbc4-6cb10d530c39",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-08T22:02:42.092243Z",
     "iopub.status.busy": "2025-12-08T22:02:42.091941Z",
     "iopub.status.idle": "2025-12-08T22:02:42.097009Z",
     "shell.execute_reply": "2025-12-08T22:02:42.095344Z",
     "shell.execute_reply.started": "2025-12-08T22:02:42.092220Z"
    }
   },
   "outputs": [],
   "source": [
    "messages.append(dict(role=\"user\", content=\"Please expand on the last point\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f8e0bf20-a5d9-4530-bdc2-6b1cb028b3ac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-08T22:02:44.944932Z",
     "iopub.status.busy": "2025-12-08T22:02:44.944264Z",
     "iopub.status.idle": "2025-12-08T22:03:10.731719Z",
     "shell.execute_reply": "2025-12-08T22:03:10.730936Z",
     "shell.execute_reply.started": "2025-12-08T22:02:44.944882Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.51 ms, sys: 9.32 ms, total: 14.8 ms\n",
      "Wall time: 25.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "response = client.chat.completions.create(\n",
    "    model=\"dummy\",  # Llama.cpp can only serve one model at a time, so this is ignored\n",
    "    messages=messages\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695d3c40-20b8-466b-9e73-9da3ff8f238f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba815fb6-c88a-4c76-b762-c88092b659d2",
   "metadata": {},
   "source": [
    "### EXERCISE: Fetch an LLM from HF and serve it with Ollama and Llama.cpp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d86acba-b65c-45e6-90c3-49715bedf93d",
   "metadata": {},
   "source": [
    "*(10 minutes)*\n",
    "\n",
    "Using the HuggingFace verion of the TinyLlama 1.1B model found here:\n",
    "\n",
    "[https://huggingface.co/TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF](https://huggingface.co/TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF)\n",
    "\n",
    "Select the `Q4_K_M` GGUF variant from the files section.  This indicates:\n",
    "\n",
    "* `Q4` - 4-bit quantization (precision reduction)\n",
    "* `K` - K-means clustering of quantization group weights\n",
    "* `M` - Medium precision quantization (trade-off in size & performance/quality)\n",
    "\n",
    "1. Create a simple 1-line `Modelfile`\n",
    "2. Create the Ollama model\n",
    "3. Run the Ollama model & experiment with a chat session\n",
    "4. Serve the GGUF model through Llama.cpp and connect to it from the Python `openai` library"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
