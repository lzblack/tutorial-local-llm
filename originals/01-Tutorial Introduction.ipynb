{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb155d6b-d8e8-469d-ae24-52f8ad975052",
   "metadata": {},
   "source": [
    "# 1 - Tutorial: Introduction to Serving Local LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d7388d-9476-48da-bb02-2fd0da497787",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"../img/llm-provider-banner.png\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74804862-02e2-468b-b944-88eb4d7052f2",
   "metadata": {},
   "source": [
    "[https://github.com/ijstokes/tutorial-local-llm/](https://github.com/ijstokes/tutorial-local-llm/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0bfd090-b4d2-4d62-8e57-bb7b6f70eba8",
   "metadata": {},
   "source": [
    "## 1.1 Tutorial Agenda\n",
    "\n",
    "1. Local environment setup (5 minutes, with background package downloading)\n",
    "2. Why would you want to run LLMs locally?  What are the economics, pros, and cons?  (5 minutes)\n",
    "3. Getting started with Ollama for local LLM execution (15 minutes + 15 minutes exercise)\n",
    "4. Navigating HuggingFace and finding the LLM of your dreams (10 minutes + 10 minute exercise)\n",
    "5. Multimodal inputs & generating structured output (10 minutes + 10 minutes exercise)\n",
    "6. Q&A (5 minutes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e4c5d68-2cdb-48d8-bd6a-245fcbbfcda6",
   "metadata": {},
   "source": [
    "## 1.2 Preamble\n",
    "\n",
    "There are a few background tasks that you should do while we go over the introduction, as they may take 2-5 minutes for your laptop to complete, depending on your current setup and the event WiFi\n",
    "\n",
    "**NOTE 1:** If you haven't completed the setup steps in the [README.md](../README.md) then please do so now -- the Python environment is 1.5-2.5GB and there are 5GB of models to download. This will likely take 10-20 minutes to complete.  \n",
    "\n",
    "[https://github.com/ijstokes/tutorial-local-llm/](https://github.com/ijstokes/tutorial-local-llm/)\n",
    "\n",
    "**NOTE 2:** For everyone who prepared your environment *before* the start of the tutorial, please take two actions now:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef024c8-cdcf-417a-b56f-9d44349c28db",
   "metadata": {},
   "source": [
    "1. Pull the latest version of the repository, as there may have been some last minute updates:\n",
    "\n",
    "```bash\n",
    "git pull\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18bc9a5d-fc57-46ba-a85d-5992fa3ddfe6",
   "metadata": {},
   "source": [
    "2. Following that, please update your environment with one of the two commands below depending on whether you use Conda/Mamba or UV:\n",
    "\n",
    "```bash\n",
    "mamba env update -f environment.yml\n",
    "```\n",
    "or\n",
    "```bash\n",
    "uv add -r requirements.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d1dda9-f687-4fc4-8155-daf1e0a34f4d",
   "metadata": {},
   "source": [
    "**Note 3:** Everyone should have a few terminal windows open, in the tutorial directory, all with the tutorial Python environment activated (see [README.md](../README.md)).  Additionally you should have a Jupyter notebook session up and running **also** using the tutorial Python environment `tutorial-local-llm`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab290e4c-5813-4b9d-89bf-d285bd234c90",
   "metadata": {},
   "source": [
    "## 1.3 Why Run Local LLMs?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8369d0-7946-41e9-b731-c0dd5d3e7f6c",
   "metadata": {},
   "source": [
    "Let's discuss why you would consider managing and running your own local LLMs\n",
    "\n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "388c7386-42ae-4f91-98cd-e43af8b143a9",
   "metadata": {},
   "source": [
    "## 1.4 How do I know if my local LLM is good enough?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e4e5b8d-8b51-4d9d-a6f5-66d26c4f5cf4",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <img src=\"https://imgs.xkcd.com/comics/goodharts_law.png\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e07c5f3-e484-44d5-9e45-5ec3fbd6f6fc",
   "metadata": {},
   "source": [
    "## 1.4 What are drivers for LLM running costs?\n",
    "\n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc049bc8-8369-49bc-87d4-ca824bdf26e8",
   "metadata": {},
   "source": [
    "## 1.5 What drives LLM SaaS costs?\n",
    "\n",
    "It's important to understand that there are different economics for SaaS LLM services vs. self-hosted\n",
    "\n",
    "* What the market can bear\n",
    "* Model training\n",
    "* Infrastructure/capital investment\n",
    "* Business costs\n",
    "* Inference costs (see 1.4 above)\n",
    "\n",
    "Some pricing resources:\n",
    "\n",
    "* [LLM Price Check](https://llmpricecheck.com/)\n",
    "* [AI Multiple LLM Pricing Analysis](https://research.aimultiple.com/llm-pricing/)\n",
    "* [Pareto Optimal Curve for LLMs](https://sanand0.github.io/llmpricing/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f1e8f3-e614-46c3-9904-739e94669278",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
