{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb155d6b-d8e8-469d-ae24-52f8ad975052",
   "metadata": {},
   "source": [
    "# 1 - Tutorial: Introduction to Serving Local LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d7388d-9476-48da-bb02-2fd0da497787",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"../img/llm-provider-banner.png\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74804862-02e2-468b-b944-88eb4d7052f2",
   "metadata": {},
   "source": [
    "[https://github.com/ijstokes/tutorial-local-llm/](https://github.com/ijstokes/tutorial-local-llm/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0bfd090-b4d2-4d62-8e57-bb7b6f70eba8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 1.1 Tutorial Agenda\n",
    "\n",
    "1. Local environment setup (5 minutes, with background package downloading)\n",
    "2. Why would you want to run LLMs locally?  What are the economics, pros, and cons?  (10 minutes)\n",
    "3. Getting started with Ollama for local LLM execution (10 minutes + 20 minutes exercise)\n",
    "4. Navigating HuggingFace and finding the LLM of your dreams (5 minutes + 5 minute exercise)\n",
    "5. Generating structured output (10 minutes + 10 minutes exercise)\n",
    "5. Offloading LLM to an AI Appliance (5 minutes + 5 minute exercise)\n",
    "7. Q&A (5 minutes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e4c5d68-2cdb-48d8-bd6a-245fcbbfcda6",
   "metadata": {},
   "source": [
    "## 1.2 Preamble\n",
    "\n",
    "There are a few background tasks that you should do while we go over the introduction, as they may take 2-5 minutes for your laptop to complete, depending on your current setup and the event WiFi\n",
    "\n",
    "**NOTE 1:** If you haven't completed the setup steps in the [README.md](../README.md) then please do so now -- the Python environment is 1.5-2.5GB and there are 5GB of models to download. This will likely take 10-20 minutes to complete.  \n",
    "\n",
    "[https://github.com/ijstokes/tutorial-local-llm/](https://github.com/ijstokes/tutorial-local-llm/)\n",
    "\n",
    "**NOTE 2:** For everyone who prepared your environment *before* the start of the tutorial, please take two actions now:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef024c8-cdcf-417a-b56f-9d44349c28db",
   "metadata": {},
   "source": [
    "1. Pull the latest version of the repository, as there may have been some last minute updates:\n",
    "\n",
    "```bash\n",
    "git pull\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18bc9a5d-fc57-46ba-a85d-5992fa3ddfe6",
   "metadata": {},
   "source": [
    "2. Following that, please update your environment with one of the two commands below depending on whether you use Conda/Mamba or UV:\n",
    "\n",
    "```bash\n",
    "mamba env update -f environment.yml\n",
    "```\n",
    "or\n",
    "```bash\n",
    "uv add -r requirements.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d1dda9-f687-4fc4-8155-daf1e0a34f4d",
   "metadata": {},
   "source": [
    "**Note 3:** Everyone should have a few terminal windows open, in the tutorial directory, all with the tutorial Python environment activated (see [README.md](../README.md)).  Additionally you should have a Jupyter notebook session up and running **also** using the tutorial Python environment `tutorial-local-llm`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab290e4c-5813-4b9d-89bf-d285bd234c90",
   "metadata": {},
   "source": [
    "## 1.3 Why Run Local LLMs?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8369d0-7946-41e9-b731-c0dd5d3e7f6c",
   "metadata": {},
   "source": [
    "Let's discuss why you would consider managing and running your own local LLMs\n",
    "\n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e07c5f3-e484-44d5-9e45-5ec3fbd6f6fc",
   "metadata": {},
   "source": [
    "## 1.2 What are drivers for LLM running costs?\n",
    "\n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "003be2ed-b354-4264-99c0-2e0e6f002b05",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 1.1 Tutorial Agenda\n",
    "\n",
    "1. Local environment setup (5-10 minutes, with background package downloading)\n",
    "2. Why would you want to run you LLMs locally? Is it even possible? How do AI Appliances fit in? (10 minutes)\n",
    "3. Navigating HuggingFace and finding the LLM of your dreams (5 minutes + 10 minute exercise)\n",
    "4. Using Ollama from the GUI and from the CLI (5 minutes + 10 minute exercise)\n",
    "5. Using the Ollama Python library and leveraging it from code (5 minutes + 10 minute exercise)\n",
    "6. Offloading LLM to an AI Appliance (5 minutes + 5 minute exercise)\n",
    "7. Next steps for AI Autonomy, Data Sovereignty, and Advanced Analytics in the age of GenAI and LLMs (5 minutes)\n",
    "8. Q&A (10 minutes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc049bc8-8369-49bc-87d4-ca824bdf26e8",
   "metadata": {},
   "source": [
    "## 1.3 What drives LLM SaaS costs?\n",
    "\n",
    "It's important to understand that there are different economics for SaaS LLM services vs. self-hosted\n",
    "\n",
    "* What the market can bear\n",
    "* Model training\n",
    "* Infrastructure/capital investment\n",
    "* Business costs\n",
    "* Inference costs (see 1.2 above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f1e8f3-e614-46c3-9904-739e94669278",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
