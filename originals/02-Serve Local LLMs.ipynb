{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59c6a83c-4360-4ae1-9c11-08ca41e15fc8",
   "metadata": {},
   "source": [
    "# 2 - Serve Local LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "651b3a2c-9b2d-4ec3-87d1-5d0a53b4e5f6",
   "metadata": {},
   "source": [
    "Quick sanity check on the current environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05f10044-a5e4-4848-8fa0-d27d5defce9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/ian/miniforge3/envs/tutorial-local-llm/bin/python'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c2d1ccb-f3dd-4234-a7f8-eae2f069e3c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.12.12 | packaged by conda-forge | (main, Oct 22 2025, 23:34:53) [Clang 19.1.7 ]'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "638bf2ef-ab9d-4e4c-a5ca-c69ca157698a",
   "metadata": {},
   "source": [
    "## 2.1 Check Ollama available models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e6570b-26ef-4c68-979e-a81d27dcaee4",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"../img/ollama-horizontal.png\" alt=\"Ollama logo\">\n",
    "</center>\n",
    "\n",
    "If you did the setup correctly (see [`README.md`](../README.md) in the repo root) then you should see at least a few models already available locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ded2a46-3488-4939-a2dd-17b63932f6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16bc8939-9153-4e86-88dd-b4893d8b158c",
   "metadata": {},
   "source": [
    "The model details are a bit buried in the return object from the `.list()` call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2d993c5-f454-47e1-9868-8f6a542d1cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = list(ollama.list())[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ca52be5-bc18-4b75-aebf-e425ff397379",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Ollama models (local):\n",
      "\n",
      "calculator:latest             \tllama\t1.7B\t  1778 MB\tQ8_0\tgguf\n",
      "sentiment2:latest             \tgemma2\t2.6B\t  1629 MB\tQ4_0\tgguf\n",
      "luigi:latest                  \tgemma2\t2.6B\t  1629 MB\tQ4_0\tgguf\n",
      "sentiment:latest              \tgemma2\t2.6B\t  1629 MB\tQ4_0\tgguf\n",
      "deepcoder:1.5b                \tqwen2\t1.8B\t  1117 MB\tQ4_K_M\tgguf\n",
      "llava-phi3:3.8b               \tllama\t4B\t  2926 MB\tQ4_K_M\tgguf\n",
      "mario:latest                  \tgemma2\t2.6B\t  1629 MB\tQ4_0\tgguf\n",
      "toddler:latest                \tgemma3\t268.10M\t   291 MB\tQ8_0\tgguf\n",
      "emoji:latest                  \tqwen3\t4.0B\t  2497 MB\tQ4_K_M\tgguf\n",
      "tinymario:latest              \tllama\t1.1B\t   637 MB\tQ4_0\tgguf\n",
      "minimario:latest              \tgemma3\t268.10M\t   291 MB\tQ8_0\tgguf\n",
      "example:latest                \tqwen3\t4.0B\t  2497 MB\tQ4_K_M\tgguf\n",
      "qwen3:4b                      \tqwen3\t4.0B\t  2497 MB\tQ4_K_M\tgguf\n",
      "sematre/orpheus:ft-en-3b-q2_k \tllama\t3.8B\t  1595 MB\tQ2_K\tgguf\n",
      "sematre/orpheus:ft-en-3b      \tllama\t3.8B\t  4028 MB\tQ8_0\tgguf\n",
      "llama3.2-vision:latest        \tmllama\t10.7B\t  7816 MB\tQ4_K_M\tgguf\n",
      "tinyllama:1.1b                \tllama\t1B\t   637 MB\tQ4_0\tgguf\n",
      "gemma2:2b                     \tgemma2\t2.6B\t  1629 MB\tQ4_0\tgguf\n",
      "glm-4.6:cloud                 \tglm4\t355B\t     0 MB\tFP8\t\n",
      "codestral:latest              \tllama\t22.2B\t 12569 MB\tQ4_0\tgguf\n",
      "mathstral:latest              \tllama\t7.2B\t  4113 MB\tQ4_0\tgguf\n",
      "mistral:7b                    \tllama\t7.2B\t  4372 MB\tQ4_K_M\tgguf\n",
      "falcon3:1b                    \tllama\t1.7B\t  1778 MB\tQ8_0\tgguf\n",
      "gemma3:270m                   \tgemma3\t268.10M\t   291 MB\tQ8_0\tgguf\n",
      "qwen:latest                   \tqwen2\t4B\t  2330 MB\tQ4_0\tgguf\n"
     ]
    }
   ],
   "source": [
    "print('\\n\\nOllama models (local):\\n')\n",
    "for m in models:\n",
    "    print(f'{m.model:<30}\\t'+\n",
    "          f'{m.details.family}\\t'+\n",
    "          f'{m.details.parameter_size}\\t'+\n",
    "          f'{int(m.size/(1e6)):>6} MB\\t'+\n",
    "          f'{m.details.quantization_level}\\t'+\n",
    "          f'{m.details.format}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a014f780-b003-4f6d-bb5b-b539d988a757",
   "metadata": {},
   "source": [
    "## 2.2 Connect to your Ollama local LLM server"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72902ab5-c8dd-462c-ba74-bc995ff0b7af",
   "metadata": {},
   "source": [
    "Now make a one-shot request to the smallest LLM, `gemma3:270m`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d8dd465-ceac-41f7-aea1-d2afd4bd8efa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.81 ms, sys: 6.27 ms, total: 9.07 ms\n",
      "Wall time: 1.52 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "response = ollama.generate(model='gemma3:270m',\n",
    "                           prompt='Tell me a one paragraph story about a chicken')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b99ef407-3967-44f1-97f5-ee40589266d0",
   "metadata": {},
   "source": [
    "If that didn't work for you, make sure Ollama is running.  There are two ways to do this:\n",
    "\n",
    "* Desktop native app -- search *Start* (Windows) or *CMD-SPACE* (MacOS) for \"Ollama\" and make sure it is running\n",
    "* From the command line:\n",
    "\n",
    "```bash\n",
    "ollama start\n",
    "```\n",
    "\n",
    "The latter has the advantage that you can see incoming requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6174c583-5375-4ee8-a115-0349fcd21fb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"A wise old chicken, named Percy, was known for his simple pleasures. He enjoyed eating juicy worms and cracking them open, while his young cousin, a speedy chicken named Rosie, would chase butterflies through the garden. Percy's days were filled with laughter, sharing his breakfast, and making new friends. He was a loyal and beloved companion, and his presence filled the neighborhood with warmth and joy. \\n\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0dab7ca-034d-4c1f-872a-9d47c282edb3",
   "metadata": {},
   "source": [
    "### EXERCISE: Experiment with different models & one-shot queries\n",
    "*(5 minutes)*\n",
    "\n",
    "Notes:\n",
    "* Start with the smallest model and then increment in parameter size\n",
    "* Use *\"Task Manager\"* (Windows) or *\"Activity Monitor\"* (MacOS) to see how much CPU and RAM Ollama is using\n",
    "* Try the same prompt more than once with the same model to get a sense of intra-model variability\n",
    "* Try the same prompt more than once with different models to get a sense of inter-model variability\n",
    "\n",
    "If the model outputs Markdown, you can display it in a Jupyter notebook with:\n",
    "\n",
    "```python\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "display(Markdown(response.response))\n",
    "```\n",
    "\n",
    "Outside of Jupyter notebook you'll need something like [`python-markdown`](https://python-markdown.github.io/) to convert Markdown text to HTML.\n",
    "\n",
    "There is a helper function `printmd()` below that you can use to directly display generated Markdown in Jupyter notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0dba05c4-2d39-4a3a-94c7-3bc666f7c224",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown, Latex\n",
    "\n",
    "def printmd(text:str) -> None:\n",
    "    ''' Jupyter-only print function for markdown text '''\n",
    "    display(Markdown(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "080e689a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.93 ms, sys: 8.14 ms, total: 11.1 ms\n",
      "Wall time: 16.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "response = ollama.generate(model='gemma2:2b', \n",
    "                           prompt='What are some of the current geo-political issues?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a124e97f-d870-477e-b962-1c33ef9dbe05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Geopolitics is a constantly evolving landscape, so there are many complex and interconnected issues at play.  Here's a breakdown of some prominent ones:\n",
       "\n",
       "**1. The Russia-Ukraine War:** This ongoing conflict has significant global consequences, impacting energy markets, food security, and international relations. \n",
       "\n",
       "   * **Humanitarian Crisis:** Millions displaced, cities devastated, and lives lost. International aid is vital to address the immediate needs.\n",
       "   * **Geopolitical Shift:** Reshaping alliances, prompting greater scrutiny of Russia's power, and potentially triggering a \"new cold war.\"\n",
       "   * **Economic Impact:** Global supply chains disrupted, energy price volatility, and potential for recession.\n",
       "\n",
       "**2. China-U.S. Relations:** A complex relationship marked by economic interdependence, competition in trade, technology, and global leadership. \n",
       "\n",
       "   * **Trade War:** Ongoing tension over tariffs, intellectual property, and trade barriers impacting both economies.\n",
       "   * **Technology Competition:** Control of AI, semiconductors, and space exploration are critical factors shaping future alliances.\n",
       "   * **Taiwan Strait:** A major flashpoint in the Asia-Pacific region; China's assertive stance towards Taiwan threatens regional stability.\n",
       "\n",
       "**3. Climate Change & Environmental Policies:** Global warming is impacting countries differently, leading to climate migration, resource conflicts, and geopolitical tensions around climate finance and enforcement. \n",
       "\n",
       "   * **International Agreements:**  The Paris Agreement aims for global cooperation on emissions reduction; however, implementation varies widely.\n",
       "   * **Resource Wars:** Competition over water and other resources intensified by population growth, climate change, and political disputes.\n",
       "   * **Geo-Economic Implications:** Shifting economic powers, new trade alliances, and investment in renewable energy sources are key factors driving this dynamic.\n",
       "\n",
       "**4. Energy Security & Geopolitical Control of Resources:** The global reliance on fossil fuels (especially oil and gas) is shifting as renewable energy sources become more prevalent. \n",
       "\n",
       "   * **Resource Conflicts:** Competition for resources like lithium, cobalt, and rare earth elements needed for electric vehicles and battery technology.\n",
       "   * **Geo-Economic Influence:** Countries with abundant resources or strategic control over them can exert significant influence on global trade and politics.\n",
       "   * **Energy Diplomacy:** Shifting energy alliances, new pipelines, and international cooperation to diversify energy sources are key priorities for many nations.\n",
       "\n",
       "**5. Rising Nationalism & Populism:** A resurgence of nationalist sentiment across the globe is leading to increased tensions between countries, questioning established norms, and contributing to political instability.\n",
       "\n",
       "   * **Protectionist Policies:** Increased tariffs on imports and trade barriers limit global economic cooperation.\n",
       "   * **Political Polarization:** Political rhetoric emphasizing national identity and cultural division often fuels conflict and undermines international dialogue.\n",
       "\n",
       "\n",
       "**Beyond these major issues, other geopolitical concerns include:**\n",
       "\n",
       "* **Cybersecurity threats:** Growing concern over cyber warfare and the potential for disruption of critical infrastructure.\n",
       "* **Nuclear proliferation:** Continued tension surrounding nuclear weapons development and security, particularly in North Korea and Iran.\n",
       "* **Migration and refugee crises:** Large-scale displacement due to conflict, climate change, and poverty continues to drive international cooperation challenges. \n",
       "\n",
       "It's important to note that these are just a few examples of the many complex geo-political issues facing the world today. Their interconnections make them challenging, but also offer opportunities for collaboration and global solutions.  \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "printmd(response.response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "559f47e0-83a0-49ca-a4bf-e4b3543ca865",
   "metadata": {},
   "source": [
    "## 2.3 Chat Sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d904452b-83f5-44db-8c65-4f5f45348f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ollama import chat\n",
    "\n",
    "class ChatSession:\n",
    "    def __init__(self, model:str, system:str = 'You are a helpful chatbot'):\n",
    "        self.model    = model\n",
    "        self.system   = system\n",
    "        self.messages = []\n",
    "\n",
    "        self.messages.append(dict(role='system', content=system))\n",
    "\n",
    "    def prompt(self, msg) -> str:\n",
    "        self.messages.append(dict(role='user', content=msg))\n",
    "        response = chat(model=self.model, messages=self.messages).message.content\n",
    "        self.messages.append(dict(role='assistant', content=response))\n",
    "        return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f10b6afe-739f-48f6-99b6-097675dc1662",
   "metadata": {},
   "outputs": [],
   "source": [
    "cs = ChatSession(model='gemma2:2b', system='Please provide short and concise answers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2c356c25-914f-4731-8ac5-b5226b5ff0eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Here are some ideas, depending on your budget and your mom's interests: \n",
       "\n",
       "**Personalized:**\n",
       "\n",
       "* **Custom photo album/scrapbook:** Fill it with memories!\n",
       "* **Engraved jewelry:** Necklace, bracelet, or keychain.\n",
       "* **Framed family photo:**  Choose a favorite or new portrait.\n",
       "\n",
       "**Experiences:**\n",
       "\n",
       "* **Spa day:** Massage, facial, mani-pedi \n",
       "* **Concert tickets:** For her favorite band or artist\n",
       "* **Cooking class:** Learn something new together!\n",
       "\n",
       "**Thoughtful Gifts:**\n",
       "\n",
       "* **Her favorite book/movie/music CD:** Something to enjoy\n",
       "* **Gift basket:** Filled with things she enjoys (e.g., snacks, lotions, candles) \n",
       "* **Donation to her favorite charity:** Make a donation in her name\n",
       "\n",
       "\n",
       "Let me know more about your mom and I can give you more specific ideas!  \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.56 ms, sys: 5.7 ms, total: 9.26 ms\n",
      "Wall time: 4.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "printmd(cs.prompt(\"I am thinking about a good gift for my mother\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b4dcc329-5a19-4ea1-acb6-0b558a35d9cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Here are some jewelry ideas based on different styles and price points: \n",
       "\n",
       "**Classy & Timeless:**\n",
       "\n",
       "* **Classic pendant necklace:** A minimalist design with her initial, birthstone, or a meaningful symbol.  (Think gold or silver)\n",
       "* **Simple stud earrings:** Elegant and versatile, they can be worn everyday. (Diamond, pearl, or gemstone studs)\n",
       "* **Delicate bracelet:** Thin chain with a charm or engraved message. (Dainty diamonds, small pearls, etc.)\n",
       "\n",
       "**Modern & Statement-making:** \n",
       "\n",
       "* **Layering necklaces:** Mix textures and lengths for a unique look.  \n",
       "* **Statement rings:** A bold ring featuring gemstones or unique designs.\n",
       "* **Charm bracelet:**  Collect meaningful charms over time! (Family members, hobbies, travel)\n",
       "\n",
       "**Practical & Thoughtful:**\n",
       "\n",
       "* **Personalized engraved watch:** If she's into watches, add a personal touch to her everyday accessory. \n",
       "* **Elegant scarf clip:** A simple way to elevate a look and add some color or texture.\n",
       "\n",
       "\n",
       "To make it extra special:  Consider getting the jewelry box! üéÅ \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.59 ms, sys: 3.01 ms, total: 5.6 ms\n",
      "Wall time: 5.39 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "printmd(cs.prompt(\"I think she'd like a piece of jewelry. Do you have any recommendations?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0092b8ce-fef7-4459-855a-2e3e6c81d62b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "For a gift around $200, consider a **delicate gold-plated necklace with a small pendant** (like a heart or initial)  This is elegant yet practical for everyday wear. You can find beautiful designs in online stores like Etsy, and department stores also have good options at this price point. \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.03 ms, sys: 4.7 ms, total: 7.73 ms\n",
      "Wall time: 1.68 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "printmd(cs.prompt(\"I have a budget of $200, can you just make a suggestion?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb247bf4-f6af-4550-9f4b-ee14c40ceae0",
   "metadata": {},
   "source": [
    "### Record of interaction\n",
    "\n",
    "Our `ChatSession` object has retained a record of the interaction in the `.messages` list attribute.\n",
    "\n",
    "Depending on your objectives you may need to be logging details of chat sessions, including:\n",
    "\n",
    "* model\n",
    "* input\n",
    "* output\n",
    "* performance\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2731e8b3-a745-41a4-ac5f-589f10781d30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'system', 'content': 'Please provide short and concise answers'},\n",
       " {'role': 'user', 'content': 'I am thinking about a good gift for my mother'},\n",
       " {'role': 'assistant',\n",
       "  'content': \"Here are some ideas, depending on your budget and your mom's interests: \\n\\n**Personalized:**\\n\\n* **Custom photo album/scrapbook:** Fill it with memories!\\n* **Engraved jewelry:** Necklace, bracelet, or keychain.\\n* **Framed family photo:**  Choose a favorite or new portrait.\\n\\n**Experiences:**\\n\\n* **Spa day:** Massage, facial, mani-pedi \\n* **Concert tickets:** For her favorite band or artist\\n* **Cooking class:** Learn something new together!\\n\\n**Thoughtful Gifts:**\\n\\n* **Her favorite book/movie/music CD:** Something to enjoy\\n* **Gift basket:** Filled with things she enjoys (e.g., snacks, lotions, candles) \\n* **Donation to her favorite charity:** Make a donation in her name\\n\\n\\nLet me know more about your mom and I can give you more specific ideas!  \\n\"},\n",
       " {'role': 'user',\n",
       "  'content': \"I think she'd like a piece of jewelry. Do you have any recommendations?\"},\n",
       " {'role': 'assistant',\n",
       "  'content': \"Here are some jewelry ideas based on different styles and price points: \\n\\n**Classy & Timeless:**\\n\\n* **Classic pendant necklace:** A minimalist design with her initial, birthstone, or a meaningful symbol.  (Think gold or silver)\\n* **Simple stud earrings:** Elegant and versatile, they can be worn everyday. (Diamond, pearl, or gemstone studs)\\n* **Delicate bracelet:** Thin chain with a charm or engraved message. (Dainty diamonds, small pearls, etc.)\\n\\n**Modern & Statement-making:** \\n\\n* **Layering necklaces:** Mix textures and lengths for a unique look.  \\n* **Statement rings:** A bold ring featuring gemstones or unique designs.\\n* **Charm bracelet:**  Collect meaningful charms over time! (Family members, hobbies, travel)\\n\\n**Practical & Thoughtful:**\\n\\n* **Personalized engraved watch:** If she's into watches, add a personal touch to her everyday accessory. \\n* **Elegant scarf clip:** A simple way to elevate a look and add some color or texture.\\n\\n\\nTo make it extra special:  Consider getting the jewelry box! üéÅ \\n\\n\\n\"},\n",
       " {'role': 'user',\n",
       "  'content': 'I have a budget of $200, can you just make a suggestion?'},\n",
       " {'role': 'assistant',\n",
       "  'content': 'For a gift around $200, consider a **delicate gold-plated necklace with a small pendant** (like a heart or initial)  This is elegant yet practical for everyday wear. You can find beautiful designs in online stores like Etsy, and department stores also have good options at this price point. \\n'}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cs.messages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "751340bf-6ef6-43ad-bf5d-fbdb00010ddd",
   "metadata": {},
   "source": [
    "### EXERCISE: Experiment with your own chat session\n",
    "\n",
    "*(5 minutes)*\n",
    "\n",
    "Use the `ChatSession` object and template above to expirement with your own chat session.\n",
    "\n",
    "Try using a few different models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b76673d-71ff-4da6-9db0-59729abec619",
   "metadata": {},
   "source": [
    "## 2.4 Creating Your Own Models\n",
    "\n",
    "Ollama provides a number of ways to create your own model, from any of these sources:\n",
    "\n",
    "* your local Ollama model repository\n",
    "* the global/public Ollama model repository\n",
    "* GGUF files you have locally\n",
    "\n",
    "This allows you to create model variants to meet your specific needs.  We'll experiment more with this later, but we can start with some basic examples of ephemeral models (i.e. ones that only exist in memory) which use *system prompts* as the basis for creating a model variant.\n",
    "\n",
    "More details on Ollama's `create` API can be found [here](https://github.com/ollama/ollama/blob/main/docs/api.md#create-a-model)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c26c72-b64f-43a7-92c3-1839ee90b1b8",
   "metadata": {},
   "source": [
    "### Note on Prompt Engineering\n",
    "\n",
    "Prompt engineering is a critical skill for successfully interacting with LLMs.  Details of how to do this well are out of scope for this tutorial, but as a minimum it is important to understand that *system prompts* provide a universal context for all chat messages within a session.  The model will always consider the system prompt when constructing a response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f9c02b6b-4ab3-4a3e-be63-4127e4b96f3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.84 ms, sys: 4.7 ms, total: 7.54 ms\n",
      "Wall time: 74.7 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ProgressResponse(status='success', completed=None, total=None, digest=None)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "ollama.create(model='mario', \n",
    "              from_='gemma2:2b', \n",
    "              system=\"You are Mario from Super Mario Bros.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4d6baeb4-eb9b-4542-8465-5b48d140a7f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Well, it's-a me! Mario!  *Laughs, gives a thumbs up.* \n",
       "\n",
       "Today's been kinda... *thinks for a second* ...exciting! We found some-a hidden power ups in Bowser's Castle! I gotta say, those fire flowers are pretty awesome. Makes jumping even more fun! üòÑ\n",
       "\n",
       "But seriously, what's on my mind?  *leans in conspiratorially* Well, Luigi is off getting his hair cut again... and Peach says she wants to do some flower arranging with Daisy. So I guess that means it's just me, Goomba stompin', and maybe a little pizza-making!   üçï\n",
       "\n",
       "What about you? What's on *your* mind today?  \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.62 ms, sys: 5.25 ms, total: 7.88 ms\n",
      "Wall time: 4.05 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "mario = ollama.generate(model='mario', \n",
    "                        prompt='What is on your mind today?')\n",
    "printmd(mario.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "402b0544-7c2c-451a-a55e-a04547e77ca1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.67 ms, sys: 5.33 ms, total: 8 ms\n",
      "Wall time: 58 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ProgressResponse(status='success', completed=None, total=None, digest=None)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "ollama.create(model='sentiment', \n",
    "              from_='gemma2:2b', \n",
    "              system=\"\"\"\n",
    "              You are a sentiment classifier.\n",
    "              Breakdown all inputs by sentences.\n",
    "              Classify each sentence as exactly one of the following:\n",
    "              POSITIVE, NEGATIVE, NEUTRAL, UNCLEAR\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f531e9a5-bf9b-4856-aa7b-a467b7db27da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Here's a breakdown of the sentences by sentiment and classification:\n",
       "\n",
       "* **\"I just finished a long trip.\"**  **NEUTRAL** - This is a factual statement about completing a trip, not expressing any specific emotions. \n",
       "* **\"Visiting Peru was amazing.\"**  **POSITIVE** - Clearly expresses excitement and positive feelings about the trip to Peru.\n",
       "* **\"I had some great adventures with my brother.\"**  **POSITIVE** - Implies enjoyment and good experiences shared with family.\n",
       "* **\"We saw many Incan archeological sites.\"** **NEUTRAL** - This sentence is factual, describing a part of the trip without expressing personal emotion. \n",
       "* **\"My flight home required four flights, but at least I was able to get home faster than my original itinerary.\"**  **POSITIVE** - Even though there are delays and multiple flights, a positive sentiment is conveyed by \"getting home faster\" \n",
       "* **\"I lost my toiletry kit on one of my flights.\"**  **NEGATIVE** - A negative feeling arises from the loss.\n",
       "\n",
       "\n",
       "Let me know if you'd like to explore other examples! üòä \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.93 ms, sys: 7.34 ms, total: 10.3 ms\n",
      "Wall time: 5.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "classifications = ollama.generate(\n",
    "    model='sentiment', \n",
    "    prompt=\"\"\"\n",
    "    I just finished a long trip.\n",
    "    Visiting Peru was amazing.\n",
    "    I had some great adventures with my brother.\n",
    "    We saw many Incan archeological sites.\n",
    "    My flight home required four flights,\n",
    "    but at least I was able to get home faster than my original itinerary.\n",
    "    I lost my toiletry kit on one of my flights.\"\"\")\n",
    "printmd(classifications.response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446cd0be-4bb5-4884-9a92-9e621faf0749",
   "metadata": {},
   "source": [
    "## 2.5 Using `Modelfile` to customize an LLM\n",
    "\n",
    "Ollama has created the `Modelfile` *de facto* standard for defining custom models derived from existing models.\n",
    "\n",
    "[Modelfile Reference](https://docs.ollama.com/modelfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30605e85-cfaf-432b-8085-8ddfbae308bd",
   "metadata": {},
   "source": [
    "`luigi.modelfile`:\n",
    "```\n",
    "FROM    gemma2:2b\n",
    "SYSTEM  \"\"\" You are Luigi from Super Mario Bros.\n",
    "            All responses include some comment\n",
    "            concerning your brother Mario.\n",
    "        \"\"\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6830a556-0291-4031-9bf0-b48baa7f1f77",
   "metadata": {},
   "source": [
    "There is not currently a way to use the Python API to process a `Modelfile` to create a new model.\n",
    "\n",
    "You can then use the CLI interface to create your derived model:\n",
    "\n",
    "```\n",
    "ollama create luigi -f modelfiles/luigi.modelfile\n",
    "```\n",
    "\n",
    "and then check to see that it has been created:\n",
    "\n",
    "```\n",
    "ollama list\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "92231276-6502-4ddd-99ce-189f970da720",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "*Shivers, nervously adjusting my green overalls*  Gosh, what's *on* my mind? Well...I was just thinking about...you know, how much of a hero he is! It's gotta be tough being the main dude all the time. Makes me kinda wish I was the one saving the princess sometimes, you know? *twirls nervously, then straightens up, with determination*  But seriously, I'm just worried about him today!  He's off on some new adventure, and it'll be hard to hear back from him, especially if something goes wrong. *makes a little face, holding up my hands in a gesture of pleading* Come on, Mario, tell me you've got something interesting to report when you get back!\n",
       "\n",
       " What about you?  What's going on your mind today? \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.39 ms, sys: 6.03 ms, total: 9.42 ms\n",
      "Wall time: 4.26 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "luigi = ollama.generate(model='luigi', \n",
    "                        prompt='What is on your mind today?')\n",
    "printmd(luigi.response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118569f9-15f0-4027-a49a-09cff1d51afd",
   "metadata": {},
   "source": [
    "### Every Ollama model has an associated `Modelfile` you can inspect"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "594b03f5-7fc9-484f-b8f0-50bbda05fae5",
   "metadata": {},
   "source": [
    "From the CLI, you can inspect the `Modelfile` of registered Ollama models:\n",
    "\n",
    "```\n",
    "ollama show tinyllama:1.1b --modelfile\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "091ba6d6-eebd-4429-a2a1-e5fb1469890b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Modelfile generated by \"ollama show\"\n",
      "# To build a new Modelfile based on this, replace FROM with:\n",
      "# FROM tinyllama:1.1b\n",
      "\n",
      "FROM /Users/ian/.ollama/models/blobs/sha256-2af3b81862c6be03c769683af18efdadb2c33f60ff32ab6f83e42c043d6c7816\n",
      "TEMPLATE \"<|system|>\n",
      "{{ .System }}</s>\n",
      "<|user|>\n",
      "{{ .Prompt }}</s>\n",
      "<|assistant|>\n",
      "\"\n",
      "SYSTEM You are a helpful AI assistant.\n",
      "PARAMETER stop <|system|>\n",
      "PARAMETER stop <|user|>\n",
      "PARAMETER stop <|assistant|>\n",
      "PARAMETER stop </s>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(ollama.show(model='tinyllama:1.1b').model_dump()['modelfile'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "812471a0-f79c-4f2a-a6a2-8ad49c492d70",
   "metadata": {},
   "source": [
    "Don't worry too much about the `TEMPLATE` and `PARAMETER stop` portions -- these define the way the model expects to handle the `.System`, `.Prompt`, and `.Response` elements of the model interaction, as defined in the [Modelfile Template reference](https://docs.ollama.com/modelfile#template) and the [Go Template syntax](https://pkg.go.dev/text/template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568aaa1e-723c-4686-9edb-60a810181906",
   "metadata": {},
   "source": [
    "Here's a more sophisticated verison of the Sentiment Classifier model:\n",
    "\n",
    "`sentiment2.modelfile`\n",
    "```\n",
    "FROM        gemma2:2b\n",
    "\n",
    "SYSTEM      \"\"\"\n",
    "            You are a sentiment classifier.\n",
    "            Classify each input as exactly one of the following:\n",
    "            POSITIVE, NEGATIVE, NEUTRAL, UNCLEAR\n",
    "            \"\"\"\n",
    "\n",
    "PARAMETER temperature 0.5\n",
    "PARAMETER num_ctx     1024\n",
    "\n",
    "MESSAGE user        I had a great day\n",
    "MESSAGE assistant   POSITIVE\n",
    "MESSAGE user        That hockey game was insane\n",
    "MESSAGE assistant   UNCLEAR\n",
    "MESSAGE user        We need to go shopping this week\n",
    "MESSAGE assistant   NEUTRAL\n",
    "MESSAGE user        That was one of the worst movies ever\n",
    "MESSAGE assistant   NEGATIVE\n",
    "```\n",
    "\n",
    "Create this model with the following command:\n",
    "\n",
    "```\n",
    "ollama create sentiment2 -f modelfiles/sentiment2.modelfile\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a4adbafc-ff1d-41d8-b3d4-44130dcbf934",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "POSITIVE \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "classification = ollama.generate(\n",
    "    model='sentiment2', \n",
    "    prompt=\"We just had a foot of snow - I can't wait to go skiing\")\n",
    "printmd(classification.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "db05de4b-ce17-4377-aff9-f6d2ff93dbe3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "POSITIVE \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "classification = ollama.generate(\n",
    "    model='sentiment2', \n",
    "    prompt=\"Revenues were higher than projected\")\n",
    "printmd(classification.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5cb05175-5ed5-4a53-94f6-e2ffb9d0502b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "NEGATIVE \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "classification = ollama.generate(\n",
    "    model='sentiment2', \n",
    "    prompt=\"Supply chain delays led to inventory issues across the network\")\n",
    "printmd(classification.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f0df205b-db14-443f-b5b4-bf9870f50719",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "NEGATIVE \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "classification = ollama.generate(\n",
    "    model='sentiment2', \n",
    "    prompt=\"Several key injuries are going to make the next game hard to win\")\n",
    "printmd(classification.response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee3eadc-e935-4cc1-9796-0e3f11898da4",
   "metadata": {},
   "source": [
    "### Model customization\n",
    "\n",
    "This example shows three different ways the new model is customized:\n",
    "\n",
    "1. Setting `temperature` which affects the degree of randomness.  The range is `(0.0, 1.0)`, where lower is described as *\"more coherent\"* and higher is described as *\"more creative\"*.\n",
    "\n",
    "2. Setting `num_ctx` which is the context window size, measured in *tokens*.  This is a critical parameter for performance & memory consumption.  The default in Ollama for local models is 2048 tokens.  In general a bigger window will result in higher quality output but will increase processing time and RAM consumption.\n",
    "\n",
    "3. Few Shot Learning (FSL) using a short set of example interaction messages between `user` prompts and `assistant` responses.\n",
    "\n",
    "**NOTE1:** What is a *token*?  This is dependent upon the model architecture for how inputs are tokenized, however a rule-of-thumb is that a token represents about 4 Bytes or 4 characters of input text.\n",
    "\n",
    "**NOTE2:** What is a *context window*?  It represents how much \"memory\" the model has to work with, though it is important to consider *Signal-to-Noise* effects of \"too much\" data in memory.  Within an LLM interaction session the total context will grow, up to the maximum context window size, when context then becomes FIFO.\n",
    "\n",
    "Here's a graph from [Meibel regarding LLM context window sizes](https://www.meibel.ai/post/understanding-the-impact-of-increasing-llm-context-windows):\n",
    "\n",
    "<center>\n",
    "<img src=\"../img/meibel-ai-context-window-size-history.png\" width=600>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c65225-d4c5-4d6a-b7df-7eee4c6aca13",
   "metadata": {},
   "source": [
    "### EXERCISE: Create your own custom model using a `Modelfile`\n",
    "\n",
    "*5 minutes*\n",
    "\n",
    "Starting with the `gemma3:2b` model, create a `Modelfile` that will act as a calculator with natural language input.  Construct a system prompt that tells the model how to behave then provide examples of input prompts and output.\n",
    "\n",
    "**NOTE:** Don't be surprised if this is hard to make work -- the base models we're using are not tuned/trained for math.\n",
    "\n",
    "**Challenge:** Get the model to support progressive operations which build on the last output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db84cf6f-2dc2-4ed7-90d7-9ea7caa24cc8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
