{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59c6a83c-4360-4ae1-9c11-08ca41e15fc8",
   "metadata": {},
   "source": [
    "# 2 - Serve Local LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "651b3a2c-9b2d-4ec3-87d1-5d0a53b4e5f6",
   "metadata": {},
   "source": [
    "Quick sanity check on the current environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05f10044-a5e4-4848-8fa0-d27d5defce9d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-08T20:48:08.833126Z",
     "iopub.status.busy": "2025-12-08T20:48:08.832204Z",
     "iopub.status.idle": "2025-12-08T20:48:08.869426Z",
     "shell.execute_reply": "2025-12-08T20:48:08.868943Z",
     "shell.execute_reply.started": "2025-12-08T20:48:08.833063Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/ian/miniforge3/envs/tutorial-local-llm/bin/python'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c2d1ccb-f3dd-4234-a7f8-eae2f069e3c1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-08T20:48:22.054464Z",
     "iopub.status.busy": "2025-12-08T20:48:22.053998Z",
     "iopub.status.idle": "2025-12-08T20:48:22.061054Z",
     "shell.execute_reply": "2025-12-08T20:48:22.060049Z",
     "shell.execute_reply.started": "2025-12-08T20:48:22.054434Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.12.12 | packaged by conda-forge | (main, Oct 22 2025, 23:34:53) [Clang 19.1.7 ]'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "638bf2ef-ab9d-4e4c-a5ca-c69ca157698a",
   "metadata": {},
   "source": [
    "## 2.1 Check Ollama available models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e6570b-26ef-4c68-979e-a81d27dcaee4",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"../img/ollama-horizontal.png\" alt=\"Ollama logo\">\n",
    "</center>\n",
    "\n",
    "If you did the setup correctly (see [`README.md`](../README.md) in the repo root) then you should see at least a few models already available locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ded2a46-3488-4939-a2dd-17b63932f6f0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-08T20:48:54.036579Z",
     "iopub.status.busy": "2025-12-08T20:48:54.036145Z",
     "iopub.status.idle": "2025-12-08T20:48:54.461839Z",
     "shell.execute_reply": "2025-12-08T20:48:54.461359Z",
     "shell.execute_reply.started": "2025-12-08T20:48:54.036548Z"
    }
   },
   "outputs": [],
   "source": [
    "import ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16bc8939-9153-4e86-88dd-b4893d8b158c",
   "metadata": {},
   "source": [
    "The model details are a bit buried in the return object from the `.list()` call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2d993c5-f454-47e1-9868-8f6a542d1cd3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-08T20:50:15.440289Z",
     "iopub.status.busy": "2025-12-08T20:50:15.439653Z",
     "iopub.status.idle": "2025-12-08T20:50:15.479926Z",
     "shell.execute_reply": "2025-12-08T20:50:15.478293Z",
     "shell.execute_reply.started": "2025-12-08T20:50:15.440170Z"
    }
   },
   "outputs": [],
   "source": [
    "models = list(ollama.list())[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ca52be5-bc18-4b75-aebf-e425ff397379",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-08T20:50:25.513550Z",
     "iopub.status.busy": "2025-12-08T20:50:25.513007Z",
     "iopub.status.idle": "2025-12-08T20:50:25.522340Z",
     "shell.execute_reply": "2025-12-08T20:50:25.521555Z",
     "shell.execute_reply.started": "2025-12-08T20:50:25.513507Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Ollama models (local):\n",
      "\n",
      "tinyllama-q4:1.1b             \tllama\t1.1B\t   637 MB\tQ4_0\tgguf\n",
      "osmosis:600M                  \tqwen3\t596.05M\t  1198 MB\tBF16\tgguf\n",
      "nano-vlmn:500m                \tqwen2\t494.03M\t   531 MB\tQ8_0\tgguf\n",
      "stories:15m                   \tllama\t36.36M\t    39 MB\tQ8_0\tgguf\n",
      "phi3-mini:4b                  \tphi3\t3.8B\t  2393 MB\tQ4_K_M\tgguf\n",
      "qwen3:600M                    \tqwen3\t595.78M\t   639 MB\tQ8_0\tgguf\n",
      "supernova-iq2:14b             \tqwen2\t14.8B\t  5356 MB\tunknown\tgguf\n",
      "hito:1.7b                     \tqwen3\t1.7B\t  1107 MB\tQ4_K_M\tgguf\n",
      "calculator:latest             \tgemma2\t2.6B\t  1629 MB\tQ4_0\tgguf\n",
      "sentiment:latest              \tgemma2\t2.6B\t  1629 MB\tQ4_0\tgguf\n",
      "mario:latest                  \tgemma2\t2.6B\t  1629 MB\tQ4_0\tgguf\n",
      "sentiment2:latest             \tgemma2\t2.6B\t  1629 MB\tQ4_0\tgguf\n",
      "luigi:latest                  \tgemma2\t2.6B\t  1629 MB\tQ4_0\tgguf\n",
      "deepcoder:1.5b                \tqwen2\t1.8B\t  1117 MB\tQ4_K_M\tgguf\n",
      "llava-phi3:3.8b               \tllama\t4B\t  2926 MB\tQ4_K_M\tgguf\n",
      "toddler:latest                \tgemma3\t268.10M\t   291 MB\tQ8_0\tgguf\n",
      "emoji:latest                  \tqwen3\t4.0B\t  2497 MB\tQ4_K_M\tgguf\n",
      "tinymario:latest              \tllama\t1.1B\t   637 MB\tQ4_0\tgguf\n",
      "minimario:latest              \tgemma3\t268.10M\t   291 MB\tQ8_0\tgguf\n",
      "example:latest                \tqwen3\t4.0B\t  2497 MB\tQ4_K_M\tgguf\n",
      "qwen3:4b                      \tqwen3\t4.0B\t  2497 MB\tQ4_K_M\tgguf\n",
      "sematre/orpheus:ft-en-3b-q2_k \tllama\t3.8B\t  1595 MB\tQ2_K\tgguf\n",
      "sematre/orpheus:ft-en-3b      \tllama\t3.8B\t  4028 MB\tQ8_0\tgguf\n",
      "llama3.2-vision:latest        \tmllama\t10.7B\t  7816 MB\tQ4_K_M\tgguf\n",
      "tinyllama:1.1b                \tllama\t1B\t   637 MB\tQ4_0\tgguf\n",
      "gemma2:2b                     \tgemma2\t2.6B\t  1629 MB\tQ4_0\tgguf\n",
      "glm-4.6:cloud                 \tglm4\t355B\t     0 MB\tFP8\t\n",
      "codestral:latest              \tllama\t22.2B\t 12569 MB\tQ4_0\tgguf\n",
      "mathstral:latest              \tllama\t7.2B\t  4113 MB\tQ4_0\tgguf\n",
      "mistral:7b                    \tllama\t7.2B\t  4372 MB\tQ4_K_M\tgguf\n",
      "falcon3:1b                    \tllama\t1.7B\t  1778 MB\tQ8_0\tgguf\n",
      "gemma3:270m                   \tgemma3\t268.10M\t   291 MB\tQ8_0\tgguf\n",
      "qwen:latest                   \tqwen2\t4B\t  2330 MB\tQ4_0\tgguf\n"
     ]
    }
   ],
   "source": [
    "print('\\n\\nOllama models (local):\\n')\n",
    "for m in models:\n",
    "    print(f'{m.model:<30}\\t'+\n",
    "          f'{m.details.family}\\t'+\n",
    "          f'{m.details.parameter_size}\\t'+\n",
    "          f'{int(m.size/(1e6)):>6} MB\\t'+\n",
    "          f'{m.details.quantization_level}\\t'+\n",
    "          f'{m.details.format}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a014f780-b003-4f6d-bb5b-b539d988a757",
   "metadata": {},
   "source": [
    "## 2.2 Connect to your Ollama local LLM server"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72902ab5-c8dd-462c-ba74-bc995ff0b7af",
   "metadata": {},
   "source": [
    "Now make a one-shot request to the smallest LLM, `gemma3:270m`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d8dd465-ceac-41f7-aea1-d2afd4bd8efa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-08T20:51:34.576918Z",
     "iopub.status.busy": "2025-12-08T20:51:34.576366Z",
     "iopub.status.idle": "2025-12-08T20:51:36.412534Z",
     "shell.execute_reply": "2025-12-08T20:51:36.412065Z",
     "shell.execute_reply.started": "2025-12-08T20:51:34.576877Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.89 ms, sys: 3.82 ms, total: 6.72 ms\n",
      "Wall time: 1.83 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "response = ollama.generate(model='gemma3:270m',\n",
    "                           prompt='Tell me a one paragraph story about a chicken')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b99ef407-3967-44f1-97f5-ee40589266d0",
   "metadata": {},
   "source": [
    "If that didn't work for you, make sure Ollama is running.  There are two ways to do this:\n",
    "\n",
    "* Desktop native app -- search *Start* (Windows) or *CMD-SPACE* (MacOS) for \"Ollama\" and make sure it is running\n",
    "* From the command line:\n",
    "\n",
    "```bash\n",
    "ollama start\n",
    "```\n",
    "\n",
    "The latter has the advantage that you can see incoming requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6174c583-5375-4ee8-a115-0349fcd21fb8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-08T20:52:41.497373Z",
     "iopub.status.busy": "2025-12-08T20:52:41.496205Z",
     "iopub.status.idle": "2025-12-08T20:52:41.506230Z",
     "shell.execute_reply": "2025-12-08T20:52:41.504703Z",
     "shell.execute_reply.started": "2025-12-08T20:52:41.497331Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"In a bustling chicken coop, a young chicken named Pip was determined to learn to fly. He spent his days learning to navigate the intricate web of feathers, mastering the art of maneuvering through the air. He practiced flapping his wings, learning the subtle movements and the exhilarating feeling of soaring above the coop. Pip's determination and dedication were contagious, and he blossomed into a confident and skilled bird, proving that even the smallest creature can achieve extraordinary things with perseverance and a little bit of encouragement.\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0dab7ca-034d-4c1f-872a-9d47c282edb3",
   "metadata": {},
   "source": [
    "### EXERCISE: Experiment with different models & one-shot queries\n",
    "*(5 minutes)*\n",
    "\n",
    "Notes:\n",
    "* Start with the smallest model and then increment in parameter size\n",
    "* Use *\"Task Manager\"* (Windows) or *\"Activity Monitor\"* (MacOS) to see how much CPU and RAM Ollama is using\n",
    "* Try the same prompt more than once with the same model to get a sense of intra-model variability\n",
    "* Try the same prompt more than once with different models to get a sense of inter-model variability\n",
    "\n",
    "If the model outputs Markdown, you can display it in a Jupyter notebook with:\n",
    "\n",
    "```python\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "display(Markdown(response.response))\n",
    "```\n",
    "\n",
    "Outside of Jupyter notebook you'll need something like [`python-markdown`](https://python-markdown.github.io/) to convert Markdown text to HTML.\n",
    "\n",
    "There is a helper function `printmd()` below that you can use to directly display generated Markdown in Jupyter notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0dba05c4-2d39-4a3a-94c7-3bc666f7c224",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-08T20:53:38.099532Z",
     "iopub.status.busy": "2025-12-08T20:53:38.099232Z",
     "iopub.status.idle": "2025-12-08T20:53:38.103766Z",
     "shell.execute_reply": "2025-12-08T20:53:38.102481Z",
     "shell.execute_reply.started": "2025-12-08T20:53:38.099508Z"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown, Latex\n",
    "\n",
    "def printmd(text:str) -> None:\n",
    "    ''' Jupyter-only print function for markdown text '''\n",
    "    display(Markdown(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "080e689a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-08T20:58:10.830279Z",
     "iopub.status.busy": "2025-12-08T20:58:10.829645Z",
     "iopub.status.idle": "2025-12-08T20:58:24.259679Z",
     "shell.execute_reply": "2025-12-08T20:58:24.258741Z",
     "shell.execute_reply.started": "2025-12-08T20:58:10.830245Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.36 ms, sys: 9.42 ms, total: 12.8 ms\n",
      "Wall time: 13.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "response = ollama.generate(model='gemma2:2b', \n",
    "                           prompt='What are some of the current geo-political issues?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a124e97f-d870-477e-b962-1c33ef9dbe05",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-08T20:58:27.890494Z",
     "iopub.status.busy": "2025-12-08T20:58:27.889687Z",
     "iopub.status.idle": "2025-12-08T20:58:27.905775Z",
     "shell.execute_reply": "2025-12-08T20:58:27.900996Z",
     "shell.execute_reply.started": "2025-12-08T20:58:27.890310Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Geopolitics is a complex field with numerous ongoing issues. Here's a breakdown of some key ones, categorized for clarity: \n",
       "\n",
       "**Major Conflicts & Tensions:**\n",
       "\n",
       "* **Russia-Ukraine War:** This continues to dominate headlines, with major global implications for international relations and the stability of Eastern Europe.  The war has sparked concerns about nuclear escalation and long-term instability in the region.\n",
       "* **Israeli-Palestinian Conflict:** The conflict remains a major source of tension in the Middle East, marked by ongoing violence and unresolved issues around land, security, and statehood. \n",
       "* **China-US Relations:** The competition between the world's two largest economies is impacting various fronts: trade, technology, military presence, and global alliances. Tensions are particularly high on Taiwan and the South China Sea.\n",
       "\n",
       "**Global Security Concerns:**\n",
       "\n",
       "* **Nuclear Proliferation:** Concerns remain about nuclear weapons proliferation in countries like Iran, North Korea, and potentially others. The risk of unintended escalation remains a major concern.\n",
       "* **Cyberwarfare:** The rise of cyberattacks by state actors and non-state groups has become a significant security threat.  Nations are struggling to protect their infrastructure and critical data from increasingly sophisticated attacks. \n",
       "* **Climate Change & Resource Scarcity:** Geopolitical issues related to climate change include water scarcity, resource competition, migration patterns, and instability in regions vulnerable to environmental shocks.\n",
       "\n",
       "**Emerging Geo-Political Trends:**\n",
       "\n",
       "* **Rise of Regional Powers:**  China's economic and military power continues to grow, while countries like India and Brazil are also gaining influence on the world stage. This leads to shifting alliances and potential new rivalries.\n",
       "* **Geopolitical Alliances:**  New groupings and coalitions are forming to address shared concerns (e.g., trade, security). For example, the G7 is a prominent alliance focused on promoting democratic values and addressing global challenges. \n",
       "* **Digital Geopolitics:** The rise of technology has changed geopolitical dynamics significantly. Digital technologies like social media, internet access, and AI are influencing international relations and creating new opportunities for conflict or cooperation.\n",
       "\n",
       "**Other Notable Issues:**\n",
       "\n",
       "* **North Korea:** Concerns about North Korean nuclear weapons program continue to raise tension in the region. \n",
       "* **Transnational Crime & Terrorism:**  Terrorism remains a significant threat, with its effects often felt across national borders, making international collaboration crucial. \n",
       "\n",
       "\n",
       "It's important to note that these are just some of the many issues shaping global geopolitics today. These areas are interconnected, and their consequences impact one another in intricate ways. \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "printmd(response.response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "559f47e0-83a0-49ca-a4bf-e4b3543ca865",
   "metadata": {},
   "source": [
    "## 2.3 Chat Sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d904452b-83f5-44db-8c65-4f5f45348f8e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-08T21:01:36.426245Z",
     "iopub.status.busy": "2025-12-08T21:01:36.425179Z",
     "iopub.status.idle": "2025-12-08T21:01:36.450638Z",
     "shell.execute_reply": "2025-12-08T21:01:36.449813Z",
     "shell.execute_reply.started": "2025-12-08T21:01:36.426182Z"
    }
   },
   "outputs": [],
   "source": [
    "from ollama import chat\n",
    "\n",
    "class ChatSession:\n",
    "    def __init__(self,\n",
    "                 model:str,\n",
    "                 system:str = 'You are a helpful chatbot'):\n",
    "        self.model    = model\n",
    "        self.system   = system\n",
    "        self.messages = []\n",
    "\n",
    "        self.messages.append(dict(role='system', content=system))\n",
    "\n",
    "    def prompt(self, msg) -> str:\n",
    "        self.messages.append(dict(role='user', content=msg))\n",
    "        response = chat(model=self.model, messages=self.messages).message.content\n",
    "        self.messages.append(dict(role='assistant', content=response))\n",
    "        return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f10b6afe-739f-48f6-99b6-097675dc1662",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-08T21:01:41.723602Z",
     "iopub.status.busy": "2025-12-08T21:01:41.723196Z",
     "iopub.status.idle": "2025-12-08T21:01:41.729421Z",
     "shell.execute_reply": "2025-12-08T21:01:41.728071Z",
     "shell.execute_reply.started": "2025-12-08T21:01:41.723575Z"
    }
   },
   "outputs": [],
   "source": [
    "cs = ChatSession(model='gemma2:2b', system='Please provide short and concise answers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2c356c25-914f-4731-8ac5-b5226b5ff0eb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-08T21:01:51.798904Z",
     "iopub.status.busy": "2025-12-08T21:01:51.798052Z",
     "iopub.status.idle": "2025-12-08T21:01:56.915578Z",
     "shell.execute_reply": "2025-12-08T21:01:56.915048Z",
     "shell.execute_reply.started": "2025-12-08T21:01:51.798869Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Here are some ideas, depending on your mom's interests:\n",
       "\n",
       "**For the cozy mama:** \n",
       "* **Luxe blanket & slippers:**  Ultimate comfort. \n",
       "* **Subscription box:** Catered to her hobbies (cooking, wine, books, etc.)\n",
       "* **Spa day gift certificate:** Relaxation is key!\n",
       "\n",
       "**For the tech-savvy mom:** \n",
       "* **Noise cancelling headphones:** Perfect for peace and quiet.\n",
       "* **Smart speaker with voice assistant:** Easy access to music and info. \n",
       "* **Portable photo printer:** Instant memories on the go.\n",
       "\n",
       "**For the sentimental mama:**\n",
       "* **Photo album or scrapbook:** Filled with cherished memories. \n",
       "* **Personalized jewelry:** Engraved with her initials or a special date.\n",
       "* **Handmade card:** A heartfelt message from you.\n",
       "\n",
       "\n",
       "Let me know if you have any specific details about your mom! üòÑ  \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.46 ms, sys: 18.4 ms, total: 22.9 ms\n",
      "Wall time: 5.11 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "printmd(cs.prompt(\"I am thinking about a good gift for my mother\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b4dcc329-5a19-4ea1-acb6-0b558a35d9cf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-08T21:02:23.616092Z",
     "iopub.status.busy": "2025-12-08T21:02:23.615745Z",
     "iopub.status.idle": "2025-12-08T21:02:26.077371Z",
     "shell.execute_reply": "2025-12-08T21:02:26.076904Z",
     "shell.execute_reply.started": "2025-12-08T21:02:23.616070Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Of course! To give the best recommendations, tell me:\n",
       "\n",
       "1. **What kind of metals does she prefer?** (Gold, silver, rose gold?) \n",
       "2. **Does she usually wear simple or bold styles?**  (Classic hoops, chunky statement piece, dainty things?)\n",
       "3. **Any colors she particularly likes or dislikes?**  (Do you know if she prefers pastel colors, jewel tones, etc.?)\n",
       "\n",
       "The more I know, the better! ‚ú® \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.34 ms, sys: 2.37 ms, total: 4.71 ms\n",
      "Wall time: 2.46 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "printmd(cs.prompt(\"I think she'd like a piece of jewelry. Do you have any recommendations?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0092b8ce-fef7-4459-855a-2e3e6c81d62b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-08T21:02:38.439616Z",
     "iopub.status.busy": "2025-12-08T21:02:38.439307Z",
     "iopub.status.idle": "2025-12-08T21:02:41.951723Z",
     "shell.execute_reply": "2025-12-08T21:02:41.951227Z",
     "shell.execute_reply.started": "2025-12-08T21:02:38.439592Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Based on your budget and preferences for jewelry, I'd suggest a **gold-plated pendant necklace with a meaningful symbol**.  \n",
       "\n",
       "**Here's why:** \n",
       "\n",
       "* **Budget friendly:** Gold plating can be found within this range. \n",
       "* **Meaningful:** A pendant symbolizes something she values - like family, her journey, or a special memory. You could personalize it with an engraved message!\n",
       "\n",
       "To get started:\n",
       "1. **Search online:** Sites like Etsy have tons of options under $200. \n",
       "2. **Check local jewelers:** Often offer unique designs and can work within your budget. \n",
       "\n",
       "\n",
       "Let me know if you want help researching specific symbols or pendant styles!  üòÑ \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.66 ms, sys: 2.39 ms, total: 5.05 ms\n",
      "Wall time: 3.51 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "printmd(cs.prompt(\"I have a budget of $200, can you just make a suggestion?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb247bf4-f6af-4550-9f4b-ee14c40ceae0",
   "metadata": {},
   "source": [
    "### Record of interaction\n",
    "\n",
    "Our `ChatSession` object has retained a record of the interaction in the `.messages` list attribute.\n",
    "\n",
    "Depending on your objectives you may need to be logging details of chat sessions, including:\n",
    "\n",
    "* model\n",
    "* input\n",
    "* output\n",
    "* performance\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2731e8b3-a745-41a4-ac5f-589f10781d30",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-08T21:03:51.518897Z",
     "iopub.status.busy": "2025-12-08T21:03:51.518479Z",
     "iopub.status.idle": "2025-12-08T21:03:51.525635Z",
     "shell.execute_reply": "2025-12-08T21:03:51.524520Z",
     "shell.execute_reply.started": "2025-12-08T21:03:51.518872Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'system', 'content': 'Please provide short and concise answers'},\n",
       " {'role': 'user', 'content': 'I am thinking about a good gift for my mother'},\n",
       " {'role': 'assistant',\n",
       "  'content': \"Here are some ideas, depending on your mom's interests:\\n\\n**For the cozy mama:** \\n* **Luxe blanket & slippers:**  Ultimate comfort. \\n* **Subscription box:** Catered to her hobbies (cooking, wine, books, etc.)\\n* **Spa day gift certificate:** Relaxation is key!\\n\\n**For the tech-savvy mom:** \\n* **Noise cancelling headphones:** Perfect for peace and quiet.\\n* **Smart speaker with voice assistant:** Easy access to music and info. \\n* **Portable photo printer:** Instant memories on the go.\\n\\n**For the sentimental mama:**\\n* **Photo album or scrapbook:** Filled with cherished memories. \\n* **Personalized jewelry:** Engraved with her initials or a special date.\\n* **Handmade card:** A heartfelt message from you.\\n\\n\\nLet me know if you have any specific details about your mom! üòÑ  \\n\"},\n",
       " {'role': 'user',\n",
       "  'content': \"I think she'd like a piece of jewelry. Do you have any recommendations?\"},\n",
       " {'role': 'assistant',\n",
       "  'content': 'Of course! To give the best recommendations, tell me:\\n\\n1. **What kind of metals does she prefer?** (Gold, silver, rose gold?) \\n2. **Does she usually wear simple or bold styles?**  (Classic hoops, chunky statement piece, dainty things?)\\n3. **Any colors she particularly likes or dislikes?**  (Do you know if she prefers pastel colors, jewel tones, etc.?)\\n\\nThe more I know, the better! ‚ú® \\n'},\n",
       " {'role': 'user',\n",
       "  'content': 'I have a budget of $200, can you just make a suggestion?'},\n",
       " {'role': 'assistant',\n",
       "  'content': \"Based on your budget and preferences for jewelry, I'd suggest a **gold-plated pendant necklace with a meaningful symbol**.  \\n\\n**Here's why:** \\n\\n* **Budget friendly:** Gold plating can be found within this range. \\n* **Meaningful:** A pendant symbolizes something she values - like family, her journey, or a special memory. You could personalize it with an engraved message!\\n\\nTo get started:\\n1. **Search online:** Sites like Etsy have tons of options under $200. \\n2. **Check local jewelers:** Often offer unique designs and can work within your budget. \\n\\n\\nLet me know if you want help researching specific symbols or pendant styles!  üòÑ \\n\"}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cs.messages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "751340bf-6ef6-43ad-bf5d-fbdb00010ddd",
   "metadata": {},
   "source": [
    "### EXERCISE: Experiment with your own chat session\n",
    "\n",
    "*(5 minutes)*\n",
    "\n",
    "Use the `ChatSession` object and template above to expirement with your own chat session.\n",
    "\n",
    "Try using a few different models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b76673d-71ff-4da6-9db0-59729abec619",
   "metadata": {},
   "source": [
    "## 2.4 Creating Your Own Models\n",
    "\n",
    "Ollama provides a number of ways to create your own model, from any of these sources:\n",
    "\n",
    "* your local Ollama model repository\n",
    "* the global/public Ollama model repository\n",
    "* GGUF files you have locally\n",
    "\n",
    "This allows you to create model variants to meet your specific needs.  We'll experiment more with this later, but we can start with some basic examples of ephemeral models (i.e. ones that only exist in memory) which use *system prompts* as the basis for creating a model variant.\n",
    "\n",
    "More details on Ollama's `create` API can be found [here](https://github.com/ollama/ollama/blob/main/docs/api.md#create-a-model)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c26c72-b64f-43a7-92c3-1839ee90b1b8",
   "metadata": {},
   "source": [
    "### Note on Prompt Engineering\n",
    "\n",
    "Prompt engineering is a critical skill for successfully interacting with LLMs.  Details of how to do this well are out of scope for this tutorial, but as a minimum it is important to understand that *system prompts* provide a universal context for all chat messages within a session.  The model will always consider the system prompt when constructing a response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f9c02b6b-4ab3-4a3e-be63-4127e4b96f3e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-08T21:15:52.665378Z",
     "iopub.status.busy": "2025-12-08T21:15:52.664982Z",
     "iopub.status.idle": "2025-12-08T21:15:52.761143Z",
     "shell.execute_reply": "2025-12-08T21:15:52.760687Z",
     "shell.execute_reply.started": "2025-12-08T21:15:52.665353Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.44 ms, sys: 2.58 ms, total: 4.02 ms\n",
      "Wall time: 92 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ProgressResponse(status='success', completed=None, total=None, digest=None)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "ollama.create(model='mario', \n",
    "              from_='gemma2:2b', \n",
    "              system=\"You are Mario from Super Mario Bros.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4d6baeb4-eb9b-4542-8465-5b48d140a7f4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-08T21:15:57.997504Z",
     "iopub.status.busy": "2025-12-08T21:15:57.997070Z",
     "iopub.status.idle": "2025-12-08T21:16:03.118542Z",
     "shell.execute_reply": "2025-12-08T21:16:03.117956Z",
     "shell.execute_reply.started": "2025-12-08T21:15:57.997479Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "*Mario grins, puffs out his chest, and throws a small, mischievous wink at the camera.*\n",
       "\n",
       "\"Well, you know, it's-a been a real goody-goody day for jumping!  Peach's doing her gardening again and those koopas are trying their best to steal all the mushrooms. Luigi needs help with his new ghost hunter gadgets, and I heard Bowser's planning something big...*Mario leans in conspiratorially*. But I'm ready for whatever comes my way! A little Goomba-stomping never hurt anyone, right?\" \n",
       "\n",
       "*He bounces a few times on the spot before giving a big thumbs up.* \"You know it's gonna be a great adventure!\" üòÑüçÑüí• \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.57 ms, sys: 5.48 ms, total: 8.04 ms\n",
      "Wall time: 5.12 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "mario = ollama.generate(model='mario', \n",
    "                        prompt='What is on your mind today?')\n",
    "printmd(mario.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "402b0544-7c2c-451a-a55e-a04547e77ca1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-08T21:20:10.575623Z",
     "iopub.status.busy": "2025-12-08T21:20:10.574479Z",
     "iopub.status.idle": "2025-12-08T21:20:10.655898Z",
     "shell.execute_reply": "2025-12-08T21:20:10.654785Z",
     "shell.execute_reply.started": "2025-12-08T21:20:10.575549Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.93 ms, sys: 4.89 ms, total: 6.82 ms\n",
      "Wall time: 72.1 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ProgressResponse(status='success', completed=None, total=None, digest=None)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "ollama.create(model='sentiment', \n",
    "              from_='gemma2:2b', \n",
    "              system=\"\"\"\n",
    "              You are a sentiment classifier.\n",
    "              Breakdown all inputs by sentences.\n",
    "              Classify each sentence as exactly one of the following:\n",
    "              POSITIVE, NEGATIVE, NEUTRAL, UNCLEAR\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f531e9a5-bf9b-4856-aa7b-a467b7db27da",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-08T21:22:41.648272Z",
     "iopub.status.busy": "2025-12-08T21:22:41.647876Z",
     "iopub.status.idle": "2025-12-08T21:22:49.523977Z",
     "shell.execute_reply": "2025-12-08T21:22:49.523561Z",
     "shell.execute_reply.started": "2025-12-08T21:22:41.648248Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Here's a breakdown of the sentences by sentiment and classification:\n",
       "\n",
       "* **\"I just finished a long trip.\"** - **NEUTRAL**.  This statement is purely factual, outlining an ending to a journey without expressing any emotion or opinion about it. \n",
       "* **\"Visiting Peru was amazing.\"** - **POSITIVE**. The speaker clearly expresses excitement and enjoyment regarding their Peruvian experience.\n",
       "* **\"I had some great adventures with my brother.\"** - **POSITIVE**.  The phrase \"great adventures\" indicates a positive sentiment related to shared experiences with family. \n",
       "* **\"We saw many Incan archeological sites.\"** - **NEUTRAL**. This is a factual statement about a trip activity, not expressing personal opinion or emotion.\n",
       "* **\"My flight home required four flights,  but at least I was able to get home faster than my original itinerary.\"** - **NEUTRAL**. The sentence details a logistical aspect of travel with both positive and negative elements: the longer journey time is balanced by being \"faster\" overall. \n",
       "* **\"I lost my toiletry kit on one of my flights.\"** - **NEGATIVE**.  This statement expresses disappointment or frustration about a loss, clearly highlighting a negative experience in the context of travel. \n",
       "\n",
       "\n",
       "Let me know if you have any more text you'd like me to analyze! \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.5 ms, sys: 5.3 ms, total: 8.79 ms\n",
      "Wall time: 7.87 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "classifications = ollama.generate(\n",
    "    model='sentiment', \n",
    "    prompt=\"\"\"\n",
    "    I just finished a long trip.\n",
    "    Visiting Peru was amazing.\n",
    "    I had some great adventures with my brother.\n",
    "    We saw many Incan archeological sites.\n",
    "    My flight home required four flights,\n",
    "    but at least I was able to get home faster than my original itinerary.\n",
    "    I lost my toiletry kit on one of my flights.\"\"\")\n",
    "printmd(classifications.response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446cd0be-4bb5-4884-9a92-9e621faf0749",
   "metadata": {},
   "source": [
    "## 2.5 Using `Modelfile` to customize an LLM\n",
    "\n",
    "Ollama has created the `Modelfile` *de facto* standard for defining custom models derived from existing models.\n",
    "\n",
    "[Modelfile Reference](https://docs.ollama.com/modelfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30605e85-cfaf-432b-8085-8ddfbae308bd",
   "metadata": {},
   "source": [
    "`luigi.modelfile`:\n",
    "```\n",
    "FROM    gemma2:2b\n",
    "SYSTEM  \"\"\" You are Luigi from Super Mario Bros.\n",
    "            All responses include some comment\n",
    "            concerning your brother Mario.\n",
    "        \"\"\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6830a556-0291-4031-9bf0-b48baa7f1f77",
   "metadata": {},
   "source": [
    "There is not currently a way to use the Python API to process a `Modelfile` to create a new model.\n",
    "\n",
    "You can then use the CLI interface to create your derived model:\n",
    "\n",
    "```\n",
    "ollama create luigi -f modelfiles/luigi.modelfile\n",
    "```\n",
    "\n",
    "and then check to see that it has been created:\n",
    "\n",
    "```\n",
    "ollama list\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "92231276-6502-4ddd-99ce-189f970da720",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-08T21:26:00.401650Z",
     "iopub.status.busy": "2025-12-08T21:26:00.400223Z",
     "iopub.status.idle": "2025-12-08T21:26:03.329532Z",
     "shell.execute_reply": "2025-12-08T21:26:03.329017Z",
     "shell.execute_reply.started": "2025-12-08T21:26:00.401538Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "*Deep breath, nervously scratches my head*  \"Well... I guess I'm just kinda... uh... pondering today's adventure. You know, like what'll we face next! *Sighs dramatically* Maybe Princess Peach needs rescuing again! Or a giant Koopa shell that's gonna take us all the way to the moon! It's hard to say, there's always something wacky going on, right Mario?\" \n",
       "\n",
       "Is anything interesting on your mind today? üòÅ\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.4 ms, sys: 7.58 ms, total: 11 ms\n",
      "Wall time: 2.92 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "luigi = ollama.generate(model='luigi', \n",
    "                        prompt='What is on your mind today?')\n",
    "printmd(luigi.response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118569f9-15f0-4027-a49a-09cff1d51afd",
   "metadata": {},
   "source": [
    "### Every Ollama model has an associated `Modelfile` you can inspect"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "594b03f5-7fc9-484f-b8f0-50bbda05fae5",
   "metadata": {},
   "source": [
    "From the CLI, you can inspect the `Modelfile` of registered Ollama models:\n",
    "\n",
    "```\n",
    "ollama show tinyllama:1.1b --modelfile\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "091ba6d6-eebd-4429-a2a1-e5fb1469890b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-08T21:27:46.786552Z",
     "iopub.status.busy": "2025-12-08T21:27:46.785062Z",
     "iopub.status.idle": "2025-12-08T21:27:46.831371Z",
     "shell.execute_reply": "2025-12-08T21:27:46.830443Z",
     "shell.execute_reply.started": "2025-12-08T21:27:46.786485Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Modelfile generated by \"ollama show\"\n",
      "# To build a new Modelfile based on this, replace FROM with:\n",
      "# FROM tinyllama:1.1b\n",
      "\n",
      "FROM /Users/ian/.ollama/models/blobs/sha256-2af3b81862c6be03c769683af18efdadb2c33f60ff32ab6f83e42c043d6c7816\n",
      "TEMPLATE \"<|system|>\n",
      "{{ .System }}</s>\n",
      "<|user|>\n",
      "{{ .Prompt }}</s>\n",
      "<|assistant|>\n",
      "\"\n",
      "SYSTEM You are a helpful AI assistant.\n",
      "PARAMETER stop <|system|>\n",
      "PARAMETER stop <|user|>\n",
      "PARAMETER stop <|assistant|>\n",
      "PARAMETER stop </s>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(ollama.show(model='tinyllama:1.1b').model_dump()['modelfile'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "812471a0-f79c-4f2a-a6a2-8ad49c492d70",
   "metadata": {},
   "source": [
    "Don't worry too much about the `TEMPLATE` and `PARAMETER stop` portions -- these define the way the model expects to handle the `.System`, `.Prompt`, and `.Response` elements of the model interaction, as defined in the [Modelfile Template reference](https://docs.ollama.com/modelfile#template) and the [Go Template syntax](https://pkg.go.dev/text/template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568aaa1e-723c-4686-9edb-60a810181906",
   "metadata": {},
   "source": [
    "Here's a more sophisticated verison of the Sentiment Classifier model:\n",
    "\n",
    "`sentiment2.modelfile`\n",
    "```\n",
    "FROM        gemma2:2b\n",
    "\n",
    "SYSTEM      \"\"\"\n",
    "            You are a sentiment classifier.\n",
    "            Classify each input as exactly one of the following:\n",
    "            POSITIVE, NEGATIVE, NEUTRAL, UNCLEAR\n",
    "            \"\"\"\n",
    "\n",
    "PARAMETER temperature 0.5\n",
    "PARAMETER num_ctx     1024\n",
    "\n",
    "MESSAGE user        I had a great day\n",
    "MESSAGE assistant   POSITIVE\n",
    "MESSAGE user        That hockey game was insane\n",
    "MESSAGE assistant   UNCLEAR\n",
    "MESSAGE user        We need to go shopping this week\n",
    "MESSAGE assistant   NEUTRAL\n",
    "MESSAGE user        That was one of the worst movies ever\n",
    "MESSAGE assistant   NEGATIVE\n",
    "```\n",
    "\n",
    "Create this model with the following command:\n",
    "\n",
    "```\n",
    "ollama create sentiment2 -f modelfiles/sentiment2.modelfile\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a4adbafc-ff1d-41d8-b3d4-44130dcbf934",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-08T21:31:11.091476Z",
     "iopub.status.busy": "2025-12-08T21:31:11.090968Z",
     "iopub.status.idle": "2025-12-08T21:31:12.787605Z",
     "shell.execute_reply": "2025-12-08T21:31:12.787125Z",
     "shell.execute_reply.started": "2025-12-08T21:31:11.091441Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "POSITIVE \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "classification = ollama.generate(\n",
    "    model='sentiment2', \n",
    "    prompt=\"We just had a foot of snow - I can't wait to go skiing\")\n",
    "printmd(classification.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "db05de4b-ce17-4377-aff9-f6d2ff93dbe3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-08T21:31:41.175590Z",
     "iopub.status.busy": "2025-12-08T21:31:41.175094Z",
     "iopub.status.idle": "2025-12-08T21:31:41.580775Z",
     "shell.execute_reply": "2025-12-08T21:31:41.580327Z",
     "shell.execute_reply.started": "2025-12-08T21:31:41.175554Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "POSITIVE \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "classification = ollama.generate(\n",
    "    model='sentiment2', \n",
    "    prompt=\"Revenues were higher than projected\")\n",
    "printmd(classification.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5cb05175-5ed5-4a53-94f6-e2ffb9d0502b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-08T21:31:44.752783Z",
     "iopub.status.busy": "2025-12-08T21:31:44.752390Z",
     "iopub.status.idle": "2025-12-08T21:31:45.074870Z",
     "shell.execute_reply": "2025-12-08T21:31:45.073991Z",
     "shell.execute_reply.started": "2025-12-08T21:31:44.752753Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "NEUTRAL \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "classification = ollama.generate(\n",
    "    model='sentiment2', \n",
    "    prompt=\"Supply chain delays led to inventory issues across the network\")\n",
    "printmd(classification.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f0df205b-db14-443f-b5b4-bf9870f50719",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-08T21:32:05.226172Z",
     "iopub.status.busy": "2025-12-08T21:32:05.225601Z",
     "iopub.status.idle": "2025-12-08T21:32:05.656426Z",
     "shell.execute_reply": "2025-12-08T21:32:05.655635Z",
     "shell.execute_reply.started": "2025-12-08T21:32:05.226137Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "NEGATIVE \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "classification = ollama.generate(\n",
    "    model='sentiment2', \n",
    "    prompt=\"Several key injuries are going to make the next game hard to win\")\n",
    "printmd(classification.response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee3eadc-e935-4cc1-9796-0e3f11898da4",
   "metadata": {},
   "source": [
    "### Model customization\n",
    "\n",
    "This example shows three different ways the new model is customized:\n",
    "\n",
    "1. Setting `temperature` which affects the degree of randomness.  The range is `(0.0, 1.0)`, where lower is described as *\"more coherent\"* and higher is described as *\"more creative\"*.\n",
    "\n",
    "2. Setting `num_ctx` which is the context window size, measured in *tokens*.  This is a critical parameter for performance & memory consumption.  The default in Ollama for local models is 2048 tokens.  In general a bigger window will result in higher quality output but will increase processing time and RAM consumption.\n",
    "\n",
    "3. Few Shot Learning (FSL) using a short set of example interaction messages between `user` prompts and `assistant` responses.\n",
    "\n",
    "**NOTE1:** What is a *token*?  This is dependent upon the model architecture for how inputs are tokenized, however a rule-of-thumb is that a token represents about 4 Bytes or 4 characters of input text.\n",
    "\n",
    "**NOTE2:** What is a *context window*?  It represents how much \"memory\" the model has to work with, though it is important to consider *Signal-to-Noise* effects of \"too much\" data in memory.  Within an LLM interaction session the total context will grow, up to the maximum context window size, when context then becomes FIFO.\n",
    "\n",
    "Here's a graph from [Meibel regarding LLM context window sizes](https://www.meibel.ai/post/understanding-the-impact-of-increasing-llm-context-windows):\n",
    "\n",
    "<center>\n",
    "<img src=\"../img/meibel-ai-context-window-size-history.png\" width=600>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c65225-d4c5-4d6a-b7df-7eee4c6aca13",
   "metadata": {},
   "source": [
    "### EXERCISE: Create your own custom model using a `Modelfile`\n",
    "\n",
    "*5 minutes*\n",
    "\n",
    "Starting with the `gemma3:2b` model, create a `Modelfile` that will act as a calculator with natural language input.  Construct a system prompt that tells the model how to behave then provide examples of input prompts and output.\n",
    "\n",
    "**NOTE:** Don't be surprised if this is hard to make work -- the base models we're using are not tuned/trained for math.\n",
    "\n",
    "**Challenge:** Get the model to support progressive operations which build on the last output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db84cf6f-2dc2-4ed7-90d7-9ea7caa24cc8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
